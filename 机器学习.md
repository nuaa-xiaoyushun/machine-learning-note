# 机器学习

[课后习题答案](https://www.cnblogs.com/tsingke/p/7233399.html)



## 模型评估与选择

### 检验误差与过拟合问题

样本数a占样本总数m的比例称为错误率：
$$
E=\frac{a}{m}
$$
相对应的精度：
$$
精度=1-错误率
$$
一般机器学习在训练集上的误差称为：训练误差或检验误差

新样本误差称为：泛化误差

将自身的一些特点当作所有潜在样本都具有的性质，导致泛化能力下降即称为（过拟合）。若样本尚未学好则称为（欠拟合）。

![p1_1](C:\Users\ALIENWARE\Desktop\机器学习笔记\p1_1.png)

NP难问题是机器学习通常面临的问题。

NP难问题为证明“P=NP”的问题。机器学习中解决过拟合问题，则是通过检验误差最小化就能获得最优解。
$$
机器学习中无法不相信P\neq{NP}，所以过拟合就无法避免。
$$


### 评估方法

**1.留出法**

![P1_2留出法](C:\Users\ALIENWARE\Desktop\机器学习笔记\P1_2留出法.png)

​	需要注意，训练?测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。

**2.交叉验证法**

![QQ截图20190105211631](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105211631.png)

![QQ截图20190105211815](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105211815.png)

交叉验证法有一种特例即为：留一法（Leave-One-Out，LOO）

​	每次训练集与初始数据集相比只少一个样本，使得实际评估的模型和期望评估的用D训练处的模型很相似。这种模型往往会比较精确，但是也有致命的缺点即消耗很大。

**3.自助法**

![QQ截图20190105212933](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105212933.png)
$$
lim_{m \to \infty }\ (1-\frac{1}{m})^m
$$
​	公式中m表示m次采样，1/m表示样本每次采样能取到的概率，1-每次采样的概率通过相乘求得最终不能被采样的概率，m当m去向无穷时，即可得到结果0.386。

### 性能度量

![QQ截图20190105214406](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105214406.png)

 **1.错误率与精度**

![QQ截图20190105214647](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105214647.png)

![QQ截图20190105214715](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105214715.png)

**1.查准率、查全率与F1**

![QQ截图20190105215009](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105215009.png)



​	查准率与查全率往往成反比。

![QQ截图20190105221143](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105221143.png)

![QQ截图20190105221823](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105221823.png)

![QQ截图20190105221907](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190105221907.png)

**ROC与AUC**

![QQ截图20190106175945](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106175945.png)

​	ROC(“受试者工作特征”曲线, Receiver Operating Characteristic)，根据机器学习预测结果样例进行排序，按此顺序逐个吧样本作为正例进行预测，每次计算出两个重要量的值，分别以它们为横、纵坐标作图，就得到了“ROC曲线”。

![QQ截图20190106181338](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106181338.png)


$$
AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_{i})*(y_{i}+y_{i+1})
$$
![QQ截图20190106185147](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106185147.png)

**代价敏感错误率与代价曲线**

非均等代价：为权衡不同类型错误所造成的不同损失，为这些错误的赋值。

![QQ截图20190106194619](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106194619.png)

![QQ截图20190106194643](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106194643.png)

![QQ截图20190106211354](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106211354.png)

![QQ截图20190106211425](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106211425.png)

### 比较检验

**假设检验**

![QQ截图20190106214420](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106214420.png)

![QQ截图20190106214850](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106214850.png)

![QQ截图20190106214927](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106214927.png)

​	机器学习的泛化错误率不大于E0，否则假设可被拒绝。

![QQ截图20190106220446](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106220446.png)

**交叉验证t检验**

![QQ截图20190106220806](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190106220806.png)

## 线性模型

![QQ截图20190108133848](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108133848.png)

### 线性回归

![QQ截图20190108134313](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108134313.png)

线性归回在矩阵的情况先需满足为正定或满秩矩阵，现实生活中X^T*X往往不是满秩，需引入正则化项。

### 对数几率回归

![QQ截图20190108144054](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108144054.png)

![QQ截图20190108144155](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108144155.png)

![QQ截图20190108145156](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108145156.png)

### 线性判别分析

![QQ截图20190108154525](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108154525.png)

LDA通常被视为一种几点的监督降维技术

### 多分类学习

![QQ截图20190108165455](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108165455.png)

![QQ截图20190108170544](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108170544.png)

![QQ截图20190108173517](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108173517.png)

### 类别不平衡问题

![QQ截图20190108173638](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108173638.png)

![QQ截图20190108173852](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108173852.png)

![QQ截图20190108173937](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190108173937.png)

## 决策树

### 基本流程

​	决策树是一种常见的机器学习方法，将希望从给定训练数据集学到一个模型用以对新示例进行分类。

![QQ截图20190117161324](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190117161324.png)

​	一颗决策树包含了一个根节点、若干个内部结点和若干个叶节点；叶节点对应决策树的结果，其他每个节点则对应一个属性测试；每个节点包含的样本集合根据属性测试的结果划分到的子节点中；根据节点包含样本全集。从而根节点到每个叶节点的路径对应了一个判定测试序列。决策树学习的目的是为了产生一颗泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的“分而治之”策略。

![QQ截图20190117162133](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190117162133.png)

![QQ截图20190117162806](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190117162806.png)

### 划分选择

​	通过上述算法可知，实现决策树学习的关键是看第8行，即如何选择最优划分属性，一般而言，随着划分过程不断进行，我们希望决策树的分直接点所包含的样本就看你属于同一类别，即结点的“纯度”越来越高。

**信息增益 **

![QQ截图20190117211246](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190117211246.png)

​	Ent(D)的值越小，则D的纯度越高。

![QQ截图20190117211736](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190117211736.png)

**增益率**

![QQ截图20190117213708](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190117213708.png)

**基尼指数**

![QQ截图20190117214053](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190117214053.png)

### 剪枝处理

​	剪枝是决策树学习算法对付“过拟合”的主要手段，在决策树学习中，为了正确分类训练样本，结点划分过程将不断重复。因此要通过主动去掉一些分支来降低过拟合的风险。其主要可分为预剪枝和后剪枝：

* 预剪枝：是指在决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带决策树泛化能力的提升，则停止划分并将当前结点标记为叶节点。
* 后剪枝：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对飞叶节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化能力提升，则将该子树替换为叶节点。

**预剪枝**

![QQ截图20190119140400](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190119140400.png)

​	预剪枝使得决策树很多分支都没有“展开”，这不仅降低了过拟合的风险，而且显著地减少了决策树的训练时间开销和测试时间开销。另一方面，有些分支的当前划分谁不能提升泛化性能、甚至可能导致泛化性能暂时下降，但是在其基础上进行的后续划分却有可能导致性能显著提高；预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险。

**后剪枝**



![QQ截图20190119145816](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190119145816.png)

​	后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情况下，后剪枝决策树的欠拟合风险小，泛化性能往往优于预剪枝决策树。但是后剪枝过程是在决策树生成完全决策树之后进行的，并且要自底向上地对书中的所有非叶节点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大。

### 连续与缺失值

**连续值处理**

![QQ截图20190119170721](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190119170721.png)
$$
给定样本集D和连续属性a，假定a在D上出现了n个不同的取值，这些值从大到小进行排序，记为\{a^{1},a^{2},...,a^{n}\}
$$

$$
基于划分点t，将D分为D^{-}_{t}与D^{+}_{t}，D^{-}_{t}包含了那些属性在a上取值不大于t的样本，D^{+}_{t}包含了取值大于t的样本
$$

![QQ截图20190119171919](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190119171919.png)

![QQ截图20190119172131](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190119172131.png)

![QQ截图20190119215310](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190119215310.png)

![QQ截图20190119215736](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190119215736.png)

## 神经网络

### 神经元模型

​	神经网络是具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实物体所作出的交互反应。

​	“神经网络学习”是机器学习与神经网络这两个学科领域的交叉部分。

​	神经网络中最基本的成分是神经元“neuron”模型，即上述定义的“简单单元”。每个神经元与其他神经元相连，当“兴奋”时，就会想相连的神经元放松化学物质，从而改变这些神经元内的电位，如果某神经元的电位超过一个“阈值”，那么它就会被激活，如果兴奋起来，就向其他神经元发送化学物质。

![QQ截图20190122132624](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122132624.png)

![QQ截图20190122134056](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122134056.png)

### 感知机与多层网络

![QQ截图20190122134230](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122134230.png)

![QQ截图20190122143640](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122143640.png)

![QQ截图20190122151353](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122151353.png)

![QQ截图20190122151434](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122151434.png)

### 误差逆传播法

![QQ截图20190122151722](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122151722.png)

![QQ截图20190122151925](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122151925.png)

![QQ截图20190122184046](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122184046.png)

### 全局最小与局部最小

![QQ截图20190122210445](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122210445.png)

### 其他常见的神经网络

* **RBF网络**

  ​	除了BP外第二常用的神经网络。其是一种但隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。

![QQ截图20190122212302](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122212302.png)

* **ART网络**

![QQ截图20190122212406](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122212406.png)

![QQ截图20190122212437](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122212437.png)

* **SOM网络**

![QQ截图20190122212533](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122212533.png)

![QQ截图20190122212551](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122212551.png)

* **级联相关网络**

![QQ截图20190122213035](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122213035.png)

* **Elman网络**

![QQ截图20190122213741](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122213741.png)

* **Boltzmann机**

![QQ截图20190122213935](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122213935.png)

![QQ截图20190122214526](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122214526.png)

### 深度学习

![QQ截图20190122214949](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122214949.png)

![QQ截图20190122215537](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190122215537.png)

## 支持向量机

### 间隔与支持向量

​	分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。但是可供选择的超平面很多，所以要选择超平面对训练样本局部扰动的“容忍”性最好的。要求划分超平面所产生的分类结果是最具有鲁棒性的，对未见示例的泛化能力最强的。

![QQ截图20190128233956](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190128233956.png)

![QQ截图20190128234107](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190128234107.png)

![QQ截图20190128234313](C:\Users\ALIENWARE\Desktop\机器学习笔记\QQ截图20190128234313.png)









































